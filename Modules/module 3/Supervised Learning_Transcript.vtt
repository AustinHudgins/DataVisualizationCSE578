WEBVTT

1
00:00:00.910 --> 00:00:08.115
Hello. This is Huan Liu professor of computer science and engineering.

2
00:00:08.115 --> 00:00:15.240
I'm with the School of Computing Informatics and Decision Systems Engineering.

3
00:00:15.240 --> 00:00:22.095
I'm going to cover supervised learning and unsupervised learning.

4
00:00:22.095 --> 00:00:27.280
This lecture is about supervised learning.

5
00:00:28.100 --> 00:00:34.765
In this segment, we would like to define supervised learning.

6
00:00:34.765 --> 00:00:41.720
It is also called classification and we will describe various methods.

7
00:00:41.720 --> 00:00:44.315
These are the basic methods.

8
00:00:44.315 --> 00:00:48.875
We start with Twitter data example.

9
00:00:48.875 --> 00:00:54.070
In this dataset, there are 10 instances,

10
00:00:54.070 --> 00:00:57.705
then there are three attributes.

11
00:00:57.705 --> 00:01:03.470
And the last one is the class attribute.

12
00:01:03.470 --> 00:01:05.340
So in this case,

13
00:01:05.340 --> 00:01:13.880
we just imagine that we have collected data and for the three attributes: celebrity,

14
00:01:13.880 --> 00:01:17.645
verified account and number of followers.

15
00:01:17.645 --> 00:01:23.060
And the class level is something we would like to learn or predict.

16
00:01:23.060 --> 00:01:29.095
This user is influential or not.

17
00:01:29.095 --> 00:01:33.220
And this dataset is very important because

18
00:01:33.220 --> 00:01:38.320
some for the classification we usually have this kind of

19
00:01:38.320 --> 00:01:47.300
a format and we will have collected data and try to learn the patterns.

20
00:01:47.300 --> 00:01:50.730
So for the supervised learning,

21
00:01:50.730 --> 00:01:55.215
the process is like this.

22
00:01:55.215 --> 00:01:57.695
We have a training set,

23
00:01:57.695 --> 00:02:01.905
dataset, then we have a learning algorithm,

24
00:02:01.905 --> 00:02:03.950
after we learn the model,

25
00:02:03.950 --> 00:02:09.610
we will use the model for the testing part.

26
00:02:09.610 --> 00:02:16.815
But in reality, we will just use the model for classification test.

27
00:02:16.815 --> 00:02:20.140
However, we usually include the testing

28
00:02:20.140 --> 00:02:24.925
because we would like to see how good the model is.

29
00:02:24.925 --> 00:02:28.660
And in the early example,

30
00:02:28.660 --> 00:02:32.195
when we try to apply the model,

31
00:02:32.195 --> 00:02:36.895
we predict that any user

32
00:02:36.895 --> 00:02:43.160
is influential or not and in this example here,

33
00:02:43.160 --> 00:02:48.710
we will say after we fall our e-mail messages,

34
00:02:48.710 --> 00:02:54.860
we would like to classify some messages as spam or not.

35
00:02:54.860 --> 00:03:03.665
So, here for example we have these unlabelled instances we would like to know the class.

36
00:03:03.665 --> 00:03:06.495
But m here is the model.

37
00:03:06.495 --> 00:03:09.935
So the key is to find the mapping.

38
00:03:09.935 --> 00:03:16.505
So we call the M the smart Mabi maps x 2 y,

39
00:03:16.505 --> 00:03:19.385
y is the class.

40
00:03:19.385 --> 00:03:23.695
So for supervised learning algorithms,

41
00:03:23.695 --> 00:03:31.450
there are numerous number of and we won't be able to cover all of

42
00:03:31.450 --> 00:03:40.745
them but we will try to cover those very basic and intuitive as well as popular ones.

43
00:03:40.745 --> 00:03:43.835
There are three listed here.

44
00:03:43.835 --> 00:03:48.605
So one category it's called the classification.

45
00:03:48.605 --> 00:03:52.310
It's Decision Tree Learning and k-

46
00:03:52.310 --> 00:03:57.055
Nearest Neighbor Classifier then another category is regression.

47
00:03:57.055 --> 00:04:03.310
So well spend time on all of these three methods.

48
00:04:03.310 --> 00:04:09.230
And another reason for us to choose or start with

49
00:04:09.230 --> 00:04:14.520
these three methods because usually people use one of

50
00:04:14.520 --> 00:04:23.120
them as their baseline for a performance comparison and also all these three methods are

51
00:04:23.120 --> 00:04:32.980
intuitive and they are more understandable than other methods.

52
00:04:32.980 --> 00:04:37.055
So we start with Decision Tree Learning.

53
00:04:37.055 --> 00:04:39.920
So when we discuss Decision Tree,

54
00:04:39.920 --> 00:04:46.030
we focus on induction or induce a tree.

55
00:04:46.030 --> 00:04:52.520
So we still use an example to show you. What is a Decision Tree?

56
00:04:52.520 --> 00:04:57.160
So this is the dataset we use in

57
00:04:57.160 --> 00:05:04.200
the very beginning to illustrate the need for supervised learning.

58
00:05:04.590 --> 00:05:15.315
So this is the one of the trees we can induce from this dataset.

59
00:05:15.315 --> 00:05:18.625
And for the training data,

60
00:05:18.625 --> 00:05:27.430
we have known classes and the long the tree if a tree like

61
00:05:27.430 --> 00:05:37.655
this will be applied to those data or instances with unknown classes.

62
00:05:37.655 --> 00:05:45.900
So we say unknown classes usually we mention we talk about this influential classes.

63
00:05:45.900 --> 00:05:48.915
So the question here now is,

64
00:05:48.915 --> 00:05:58.095
obviously I can have many trees and which attribute like we have three attributes,

65
00:05:58.095 --> 00:06:03.970
which attribute should we start as the root note?

66
00:06:07.250 --> 00:06:14.365
Multiple Decision Trees as we said can be learned from the same data set.

67
00:06:14.365 --> 00:06:19.960
In this case, we can start with celebrity as the root note,

68
00:06:19.960 --> 00:06:28.625
we can also start with verified account as the root note or even the number of followers.

69
00:06:28.625 --> 00:06:37.070
And after we pick the first attribute,

70
00:06:37.070 --> 00:06:44.770
then we will try to split the tree based on this attributes' values.

71
00:06:44.770 --> 00:06:46.810
In this case celebrity,

72
00:06:46.810 --> 00:06:48.625
we have two values here.

73
00:06:48.625 --> 00:06:50.345
Yes or no.

74
00:06:50.345 --> 00:06:54.330
For Verified account, we have three values.

75
00:06:54.330 --> 00:07:00.240
We have no, yes, unknown.

76
00:07:02.420 --> 00:07:12.145
But there are so many possible trees to induce or to learn then the question is,

77
00:07:12.145 --> 00:07:17.980
when the data set is big and number of attributes is large,

78
00:07:17.980 --> 00:07:28.470
how can we find a good way to learn the tree?

79
00:07:34.290 --> 00:07:40.915
Decision trees are constructed recursively. In what sense?

80
00:07:40.915 --> 00:07:47.565
So, we can have a root node, that's the dataset,

81
00:07:47.565 --> 00:07:53.285
and let's say we use verify

82
00:07:53.285 --> 00:08:00.530
the account as the first attribute here to split the data.

83
00:08:00.530 --> 00:08:04.540
So because this attribute has three bodies;

84
00:08:04.540 --> 00:08:08.210
yes, no and unknown,

85
00:08:08.210 --> 00:08:11.250
so we can have three branches.

86
00:08:11.250 --> 00:08:16.885
And then the data is split into three subsets;

87
00:08:16.885 --> 00:08:21.285
D1, D2 and D3.

88
00:08:21.285 --> 00:08:25.995
In this case, the union of D1,

89
00:08:25.995 --> 00:08:33.720
D2 and D3 equals D. So you can see,

90
00:08:33.720 --> 00:08:39.415
after the data is split based on an attribute value,

91
00:08:39.415 --> 00:08:46.070
the subsets of data are smaller and smaller.

92
00:08:46.070 --> 00:08:49.605
So this, we call tree stump.

93
00:08:49.605 --> 00:08:58.890
To build the tree recursively means after we have these subsets of data,

94
00:08:58.890 --> 00:09:04.630
we decide if we need to continue to build another tree stump.

95
00:09:04.630 --> 00:09:06.580
So in this case,

96
00:09:06.580 --> 00:09:12.020
let's say this is yes,

97
00:09:12.020 --> 00:09:17.225
this is no, this is unknown.

98
00:09:17.225 --> 00:09:20.985
And if you check what's the data,

99
00:09:20.985 --> 00:09:23.890
you will see, this is already pure.

100
00:09:23.890 --> 00:09:27.025
But if this part is not pure, then we decide,

101
00:09:27.025 --> 00:09:30.655
"Hey, I need another split."

102
00:09:30.655 --> 00:09:36.505
So, I will use this dataset and find another attribute.

103
00:09:36.505 --> 00:09:38.390
That's another attribute.

104
00:09:38.390 --> 00:09:44.275
Now, the attributes we can choose are the remaining two.

105
00:09:44.275 --> 00:09:48.090
And one is called the celebrity,

106
00:09:48.090 --> 00:09:51.665
the other one is card number of followers.

107
00:09:51.665 --> 00:09:56.400
So let's say we use the celebrity attribute.

108
00:09:56.400 --> 00:10:01.980
Then it's yes or no.

109
00:10:02.780 --> 00:10:07.240
Yes or no.

110
00:10:07.240 --> 00:10:10.820
Then we have the D31,

111
00:10:10.820 --> 00:10:19.575
D32, because it's the D31 and D32 are the subsets of D3.

112
00:10:19.575 --> 00:10:25.740
So this, way we build the tree recursively

113
00:10:25.740 --> 00:10:34.675
and datasets and each of these subsets of data becomes smaller and smaller.

114
00:10:34.675 --> 00:10:40.420
So now, we need to know when should we stop.

115
00:10:40.420 --> 00:10:51.410
So one obvious criterion is when a branch data becomes pure,

116
00:10:51.410 --> 00:10:55.460
all instances belong to one class.

117
00:10:55.460 --> 00:10:58.315
It's pure, then we can stop.

118
00:10:58.315 --> 00:11:08.805
Then we create a leaf node under that branch with the class label of that pure class.

119
00:11:08.805 --> 00:11:11.505
But in some cases,

120
00:11:11.505 --> 00:11:17.435
if the leaf node it's not pure, we still stop.

121
00:11:17.435 --> 00:11:19.620
That's why we call it a leaf node,

122
00:11:19.620 --> 00:11:29.635
because we don't have enough data points or instances to make statistically sound split.

123
00:11:29.635 --> 00:11:39.220
In this case, then we just use the majority of the class in that node.

124
00:11:39.220 --> 00:11:47.260
So now, we know we can view the tree recursively and then we know when to stop.

125
00:11:47.260 --> 00:11:49.805
But the key now is,

126
00:11:49.805 --> 00:11:51.725
how to measure purity?

127
00:11:51.725 --> 00:11:55.105
We have to calculate it, right?

128
00:11:55.105 --> 00:11:57.090
We have to compute it.

129
00:11:57.090 --> 00:12:00.760
So usually what we use it's called entropy.

130
00:12:00.760 --> 00:12:04.480
We try to minimize the entropy.

131
00:12:04.480 --> 00:12:08.510
So over a subset of training instances,

132
00:12:08.510 --> 00:12:11.720
T, that's training instances.

133
00:12:11.720 --> 00:12:14.345
If we have a binary class,

134
00:12:14.345 --> 00:12:23.390
then all these instances can be divided into positive class or negative class.

135
00:12:23.390 --> 00:12:28.390
If there are two class values,

136
00:12:28.390 --> 00:12:31.590
we can calculate the entropy this way.

137
00:12:31.590 --> 00:12:37.090
P positive is for the number of

138
00:12:37.090 --> 00:12:46.320
instances belong to the positive class divided by total number of instances,

139
00:12:46.320 --> 00:12:50.640
and P negative is the number of

140
00:12:50.640 --> 00:12:56.260
instances belong to the negative class divided by the total number of instances.

141
00:12:56.260 --> 00:13:00.135
Total number of instances equals the total number of

142
00:13:00.135 --> 00:13:05.135
positive instances plus the total number of negative instances.

143
00:13:05.135 --> 00:13:09.560
So, here, P is the probability, right?

144
00:13:09.560 --> 00:13:12.595
So P is used twice.

145
00:13:12.595 --> 00:13:19.790
Therefore, the log, and here we used it also in the log.

146
00:13:19.790 --> 00:13:26.165
So, this is the formula we used to calculate entropy.

147
00:13:26.165 --> 00:13:28.665
Now I will give you an example.

148
00:13:28.665 --> 00:13:38.205
Let's assume for the sake of easy calculation we have 10 instances,

149
00:13:38.205 --> 00:13:46.550
seven of them belong to a positive class and three belong to a negative class.

150
00:13:46.550 --> 00:13:48.580
So in this case,

151
00:13:48.580 --> 00:13:53.875
with the node to this as seven plus and three minus,

152
00:13:53.875 --> 00:13:58.925
then we can calculate the entropy like this.

153
00:13:58.925 --> 00:14:08.285
And here, as long as we pick the log base consistently, it is easy.

154
00:14:08.285 --> 00:14:10.285
It is okay.

155
00:14:10.285 --> 00:14:13.820
You use two or you use 10.

156
00:14:13.820 --> 00:14:16.990
But for the explanation,

157
00:14:16.990 --> 00:14:18.800
usually we use two.

158
00:14:18.800 --> 00:14:22.860
You can pick any base.

159
00:14:22.860 --> 00:14:26.520
Now, in the pure subset,

160
00:14:26.520 --> 00:14:29.790
all instances have the same class attribute value.

161
00:14:29.790 --> 00:14:32.900
That means then attribute is zero.

162
00:14:32.900 --> 00:14:34.785
So what does that mean?

163
00:14:34.785 --> 00:14:37.895
So in the previous example,

164
00:14:37.895 --> 00:14:43.495
we have 10 positive, zero negative.

165
00:14:43.495 --> 00:14:49.255
Or zero negative, 10 positive.

166
00:14:49.255 --> 00:14:51.605
This is pure case.

167
00:14:51.605 --> 00:14:55.845
So now we learnt entropy calculation.

168
00:14:55.845 --> 00:15:04.705
We will use this entropy formula to evaluate a range of values.

169
00:15:04.705 --> 00:15:07.135
So what is a pure subset?

170
00:15:07.135 --> 00:15:11.740
A pure subset, we will have such a case

171
00:15:11.740 --> 00:15:18.170
like 10 positive instances and zero negative instance,

172
00:15:18.170 --> 00:15:22.535
or zero positive instances

173
00:15:22.535 --> 00:15:28.585
and 10 negative instances, in this case.

174
00:15:28.585 --> 00:15:34.020
So now, let's calculate entropy.

175
00:15:36.500 --> 00:15:47.955
10 positive, zero negative that equals P 10 over 10,

176
00:15:47.955 --> 00:15:56.550
log 10 over 10 minus zero over 10,

177
00:15:56.550 --> 00:16:00.640
log zero over 10.

178
00:16:00.640 --> 00:16:02.920
So in this case,

179
00:16:02.920 --> 00:16:05.620
log 10 over 10 is log 1,

180
00:16:05.620 --> 00:16:11.305
so it's a zero and zero times anything is zero. So it's a zero.

181
00:16:11.305 --> 00:16:20.235
Now we consider another it is the worst case for the century split.

182
00:16:20.235 --> 00:16:27.120
It is 5 positive instances and 5 negative instances.

183
00:16:27.120 --> 00:16:32.440
Basically the splitting does not help us at all.

184
00:16:32.440 --> 00:16:36.495
So what is the entropy of this case?

185
00:16:36.495 --> 00:16:45.605
So the entropy 5 positive,

186
00:16:45.605 --> 00:16:53.875
5 negative equals in this case because it's 5,5.

187
00:16:53.875 --> 00:16:56.895
So for the two terms here,

188
00:16:56.895 --> 00:17:00.430
1 and 2, they are exactly the same.

189
00:17:00.430 --> 00:17:04.065
So that's why we can just calculate once,

190
00:17:04.065 --> 00:17:07.140
it's two times this.

191
00:17:07.140 --> 00:17:13.205
The 5 minus 5 over 10,

192
00:17:13.205 --> 00:17:18.975
log 5 over 10.

193
00:17:18.975 --> 00:17:27.340
And equals two times half.

194
00:17:27.340 --> 00:17:31.445
And in this case, minus here.

195
00:17:31.445 --> 00:17:34.860
Basically when we put it up over there,

196
00:17:34.860 --> 00:17:40.360
it becomes log 2.

197
00:17:40.360 --> 00:17:44.000
So if we choose base two,

198
00:17:44.000 --> 00:17:47.575
then this is one.

199
00:17:47.575 --> 00:17:50.435
And at the end this is one.

200
00:17:50.435 --> 00:17:52.780
This is the worst case scenario.

201
00:17:52.780 --> 00:17:58.210
And of course, there are many cases,

202
00:17:58.210 --> 00:18:01.885
the entropy is between zero and the one.

203
00:18:01.885 --> 00:18:11.000
That ends our first segment about classification for this decision tree learning.