WEBVTT

1
00:00:00.000 --> 00:00:03.195
Welcome back. This is Huan Liu,

2
00:00:03.195 --> 00:00:06.480
a professor of Computer Science
and Engineering.

3
00:00:06.480 --> 00:00:14.350
Now we will cover supervised
learning evaluation.

4
00:00:18.230 --> 00:00:22.845
We would like to know amongst

5
00:00:22.845 --> 00:00:27.165
so many supervised
learning algorithms,

6
00:00:27.165 --> 00:00:31.930
which one is the
best for which case?

7
00:00:32.150 --> 00:00:34.900
In order to do this,

8
00:00:34.900 --> 00:00:38.350
we have to perform
some evaluation.

9
00:00:38.350 --> 00:00:41.340
Then we have to
answer the question,

10
00:00:41.340 --> 00:00:45.040
what is the fair
evaluation method?

11
00:00:45.200 --> 00:00:50.320
And why do we need to
perform evaluation?

12
00:00:50.750 --> 00:00:55.550
The ultimate goal
of evaluation is to

13
00:00:55.550 --> 00:00:59.570
make sure that reported
the results of

14
00:00:59.570 --> 00:01:03.290
supervised learning can be

15
00:01:03.290 --> 00:01:09.895
reproduced without
the involvement of authors.

16
00:01:09.895 --> 00:01:13.840
So then how can you achieve that?

17
00:01:16.550 --> 00:01:19.570
In the very beginning

18
00:01:20.810 --> 00:01:25.160
when people can just do
this I have a dataset

19
00:01:25.160 --> 00:01:31.490
like this and we

20
00:01:31.490 --> 00:01:36.925
say this is training data
as well as the test data.

21
00:01:36.925 --> 00:01:39.920
So we learn from
training data and

22
00:01:39.920 --> 00:01:44.670
apply the same data and to
measure the performance.

23
00:01:45.430 --> 00:01:48.540
But is this fair?

24
00:01:48.550 --> 00:01:54.360
Is this good way to
evaluate? Probably not.

25
00:01:54.360 --> 00:01:58.475
Why? Because if I just use that

26
00:01:58.475 --> 00:02:02.345
same data for testing

27
00:02:02.345 --> 00:02:06.905
then the table lookup
will be the best.

28
00:02:06.905 --> 00:02:11.090
That's number one.
Number two even if I

29
00:02:11.090 --> 00:02:15.995
learn a model through
some learning algorithm.

30
00:02:15.995 --> 00:02:20.210
I will overfit the data.

31
00:02:20.210 --> 00:02:24.215
It describes everything I have

32
00:02:24.215 --> 00:02:31.849
seen just like in
a classroom teaching,

33
00:02:32.240 --> 00:02:39.815
a professor gives the homework
the same as the exam.

34
00:02:39.815 --> 00:02:41.720
So you just learn

35
00:02:41.720 --> 00:02:44.150
the same thing and test
on the same thing.

36
00:02:44.150 --> 00:02:47.055
So it does not
encourage learning.

37
00:02:47.055 --> 00:02:51.230
But in this case because

38
00:02:51.230 --> 00:02:53.510
learning about
the future we would like

39
00:02:53.510 --> 00:02:56.000
to know for unseen cases,

40
00:02:56.000 --> 00:02:57.755
how good it is.

41
00:02:57.755 --> 00:03:00.425
Then we wouldn't divide the data.

42
00:03:00.425 --> 00:03:04.580
One way is to divide data
into threefolds then use

43
00:03:04.580 --> 00:03:10.380
two-folds for training
and one-fold for tests.

44
00:03:16.340 --> 00:03:23.685
This is why we have
training and test datasets.

45
00:03:23.685 --> 00:03:27.285
But you will say,
"Then why threefolds?"

46
00:03:27.285 --> 00:03:33.295
Not necessarily. The principle
is you cannot leave

47
00:03:33.295 --> 00:03:41.840
too much data for testing
phase because in theory,

48
00:03:41.840 --> 00:03:45.700
the more data you have
the better the model you have.

49
00:03:45.700 --> 00:03:49.420
But then we also have
a contradicting goal.

50
00:03:49.420 --> 00:03:52.345
If I use all the data
then I don't have

51
00:03:52.345 --> 00:03:56.870
any unseen data for tests.

52
00:03:57.300 --> 00:04:02.425
That's why we separate the data
into training and test.

53
00:04:02.425 --> 00:04:04.675
In this case it's one-third,

54
00:04:04.675 --> 00:04:08.785
one-third and one-third for test.

55
00:04:08.785 --> 00:04:15.250
But now we just run

56
00:04:15.250 --> 00:04:20.320
the algorithm from
the training data

57
00:04:20.320 --> 00:04:22.195
set and test on this.

58
00:04:22.195 --> 00:04:26.635
So in this case we
hide these labels.

59
00:04:26.635 --> 00:04:33.140
We don't let the running
algorithm see this part

60
00:04:34.220 --> 00:04:44.850
and we still need to handle this,

61
00:04:44.850 --> 00:04:46.965
how big the data is.

62
00:04:46.965 --> 00:04:49.815
What if I have
a very small dataset?

63
00:04:49.815 --> 00:04:53.855
Like [inaudible]
like a cancer study?

64
00:04:53.855 --> 00:04:57.670
I only have a very few
number of patients.

65
00:04:57.670 --> 00:05:01.145
It's not possible
for me to even leave

66
00:05:01.145 --> 00:05:05.395
one-third of the data
out for testing.

67
00:05:05.395 --> 00:05:08.500
So in this case we can apply this

68
00:05:08.500 --> 00:05:14.780
leave-one-out model
for evaluation.

69
00:05:14.780 --> 00:05:16.720
What do we do is,

70
00:05:16.720 --> 00:05:22.435
we just use all instances

71
00:05:22.435 --> 00:05:28.785
but one to train and the
one left out for test.

72
00:05:28.785 --> 00:05:32.265
If we have N instances,

73
00:05:32.265 --> 00:05:40.060
we use N minus 1 for
training and 1 for test.

74
00:05:47.300 --> 00:05:50.340
Then people can cheat right?

75
00:05:50.340 --> 00:05:52.475
Sometimes I just
pick the best one.

76
00:05:52.475 --> 00:05:57.845
So that's something we
should avoid. What do we do?

77
00:05:57.845 --> 00:06:01.300
We just run this N times.

78
00:06:01.300 --> 00:06:04.140
So you see, for each of

79
00:06:04.140 --> 00:06:11.325
N instances will have
a chance to be test data,

80
00:06:11.325 --> 00:06:13.575
in as the test data point.

81
00:06:13.575 --> 00:06:15.840
Okay? Then we put them

82
00:06:15.840 --> 00:06:19.975
together and calculate
the average performance.

83
00:06:19.975 --> 00:06:22.255
But this is the full dataset.

84
00:06:22.255 --> 00:06:24.595
Very, very small dataset.

85
00:06:24.595 --> 00:06:28.185
Then it is inefficient, right?

86
00:06:28.185 --> 00:06:32.905
It is also too sensitive
because for the test data,

87
00:06:32.905 --> 00:06:35.680
you either correct or not

88
00:06:35.680 --> 00:06:38.995
correct because
they only have one.

89
00:06:38.995 --> 00:06:41.550
That's why we do the k-fold.

90
00:06:41.550 --> 00:06:45.490
Usually k equals 10, for example.

91
00:06:45.490 --> 00:06:47.755
So it's called 10-fold
cross-validation.

92
00:06:47.755 --> 00:06:50.020
What do we do is this,

93
00:06:50.020 --> 00:06:53.180
it's the same idea.

94
00:06:54.710 --> 00:06:58.110
So it's one-fold, second-fold,

95
00:06:58.110 --> 00:07:04.260
tenth and in the first round

96
00:07:04.260 --> 00:07:07.560
we use nine-folds for training,

97
00:07:07.560 --> 00:07:09.450
this fold for test.

98
00:07:09.450 --> 00:07:11.220
But then we take turn.

99
00:07:11.220 --> 00:07:14.140
Next one we use

100
00:07:14.150 --> 00:07:19.315
ninth-fold for test
the rest for training.

101
00:07:19.315 --> 00:07:24.715
So each fold has its turn
for testing purpose.

102
00:07:24.715 --> 00:07:26.260
Then we add them

103
00:07:26.260 --> 00:07:30.100
together and calculate
their average performance.

104
00:07:30.100 --> 00:07:35.350
Now because each fold has
more than one instance,

105
00:07:35.350 --> 00:07:42.420
we won't be able to have
an average performance measure.

106
00:07:44.860 --> 00:07:49.955
So then what is the performance
measure we should use?

107
00:07:49.955 --> 00:07:54.755
Usually we use accuracy.

108
00:07:54.755 --> 00:08:03.405
So for a dataset let's say
this is for a test data, okay?

109
00:08:03.405 --> 00:08:07.310
The training we
already learned from

110
00:08:07.310 --> 00:08:10.280
the training data
[inaudible] and learned

111
00:08:10.280 --> 00:08:14.340
from the training data
and for this test data.

112
00:08:14.630 --> 00:08:18.715
Let's say I have 10 instances,

113
00:08:18.715 --> 00:08:22.760
then three of them wrongly

114
00:08:22.760 --> 00:08:27.335
classified and seven of
them correctly classified.

115
00:08:27.335 --> 00:08:31.845
Then in this case 3 over 10.

116
00:08:31.845 --> 00:08:34.630
So it's thirty percent

117
00:08:36.650 --> 00:08:43.310
of error rate because

118
00:08:43.310 --> 00:08:45.480
this is correctly classified,

119
00:08:45.480 --> 00:08:48.370
this incorrectly classified.

120
00:08:51.130 --> 00:08:56.305
Correctly classified.

121
00:08:56.305 --> 00:09:01.970
So you will see the accuracy
is 7 out of 10.

122
00:09:01.970 --> 00:09:09.490
So seventy percent. Accuracy is

123
00:09:09.490 --> 00:09:11.350
a very common and
intuitive way to

124
00:09:11.350 --> 00:09:13.750
evaluate and then we combine

125
00:09:13.750 --> 00:09:15.715
this performance measure with

126
00:09:15.715 --> 00:09:19.765
k-fold cross-validation
or leave one out.

127
00:09:19.765 --> 00:09:23.875
Then we will be able to produce

128
00:09:23.875 --> 00:09:30.350
reproducible performance
without others involvement.

129
00:09:30.350 --> 00:09:36.070
But then we also have
some measures like AUC.

130
00:09:36.070 --> 00:09:39.775
This is the area under ROC curve.

131
00:09:39.775 --> 00:09:43.895
Area under ROC curve.

132
00:09:43.895 --> 00:09:52.760
ROC is receiver operating
characteristic and this

133
00:09:52.760 --> 00:10:02.170
is a popular way of
measure some performance.

134
00:10:02.170 --> 00:10:05.425
Then this F-measure is

135
00:10:05.425 --> 00:10:10.830
a weighted harmonic mean of
precision and the recall.

136
00:10:10.830 --> 00:10:15.840
I think AUC or F-measure

137
00:10:15.840 --> 00:10:20.500
these measures will be

138
00:10:20.500 --> 00:10:26.270
discussed in some other
segments of this course.

139
00:10:26.280 --> 00:10:33.155
But now we know how
to use accuracy and

140
00:10:33.155 --> 00:10:35.150
k-fold cross validation to

141
00:10:35.150 --> 00:10:40.580
evaluate supervised
learning algorithms.

142
00:10:40.580 --> 00:10:43.015
Then you will say, "Hey,

143
00:10:43.015 --> 00:10:45.220
what if labels cannot be

144
00:10:45.220 --> 00:10:47.900
predicted precisely,
what should we do?"

145
00:10:47.900 --> 00:10:52.175
So we have to rely on
some measures beyond accuracy.

146
00:10:52.175 --> 00:10:54.575
One is to set a margin,

147
00:10:54.575 --> 00:10:56.825
like an error margin.

148
00:10:56.825 --> 00:11:00.410
In this case when
the observed temperature is

149
00:11:00.410 --> 00:11:03.590
71 degree any prediction range of

150
00:11:03.590 --> 00:11:06.725
71 plus minus 0.5

151
00:11:06.725 --> 00:11:10.130
can be considered as
a correct prediction.

152
00:11:10.130 --> 00:11:13.650
So you have a margin of error.

153
00:11:13.880 --> 00:11:18.365
If this is not possible then
you can do the correlation.

154
00:11:18.365 --> 00:11:21.800
Correlation is something I kept

155
00:11:21.800 --> 00:11:26.195
between predicted the labels
and the ground truths and

156
00:11:26.195 --> 00:11:31.880
if these two correlation

157
00:11:31.880 --> 00:11:35.420
can be between zero and
one highly correlated

158
00:11:35.420 --> 00:11:37.390
then it's good.

159
00:11:37.390 --> 00:11:42.760
It's not very correlated,
it's not that good.

160
00:11:43.270 --> 00:11:48.560
That's the end of supervised
learning evaluation.