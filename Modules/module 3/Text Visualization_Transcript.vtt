WEBVTT

1
00:00:00.000 --> 00:00:04.890
So, in our previous multivariate analysis modules,

2
00:00:04.890 --> 00:00:09.490
we've talked a lot about categorical data and quantitative data.

3
00:00:09.490 --> 00:00:14.225
So you're given some data set like a CSB file or an Excel table.

4
00:00:14.225 --> 00:00:17.645
We keep coming back to like our country,

5
00:00:17.645 --> 00:00:22.250
GDP, life expectancy, as you just have rows in your data set.

6
00:00:22.250 --> 00:00:26.530
Like USA has some dollars,

7
00:00:26.530 --> 00:00:28.960
and has a life expectancy of 80 years,

8
00:00:28.960 --> 00:00:30.810
where we have Sweden,

9
00:00:30.810 --> 00:00:32.970
and we have some count,

10
00:00:32.970 --> 00:00:39.600
and we have some life expectancy, and we plot these.

11
00:00:39.600 --> 00:00:43.690
But another common data set that you might have is you might just

12
00:00:43.690 --> 00:00:48.790
have a whole bunch of word documents.

13
00:00:48.790 --> 00:00:52.380
You might just have a whole file of these word documents on your computer.

14
00:00:52.380 --> 00:00:56.950
Somebody may drop you a whole bunch of files on your desk.

15
00:00:56.950 --> 00:00:59.630
You might be an intelligence analyst.

16
00:00:59.630 --> 00:01:01.500
You have a whole bunch of reports that you want to go

17
00:01:01.500 --> 00:01:03.770
through and extract information from,

18
00:01:03.770 --> 00:01:08.215
and look for how does report one relate to report two.

19
00:01:08.215 --> 00:01:11.610
Do we keep seeing this guy Ross is referred to in all of

20
00:01:11.610 --> 00:01:15.090
these different intelligence reports talking about bad guys,

21
00:01:15.090 --> 00:01:17.870
and what's his connection between all of these different bad guys?

22
00:01:17.870 --> 00:01:20.480
Or what are the different trends and themes if this

23
00:01:20.480 --> 00:01:23.340
is a bunch of different books by the same author?

24
00:01:23.340 --> 00:01:27.200
Can we detect any patterns in the way the author writes?

25
00:01:27.200 --> 00:01:31.405
Are there any common themes among his or her stories?

26
00:01:31.405 --> 00:01:36.345
So, this is a very common sort of data set you might find.

27
00:01:36.345 --> 00:01:38.260
And so we want to start thinking about what are

28
00:01:38.260 --> 00:01:41.760
the attributes of this multivariate data set,

29
00:01:41.760 --> 00:01:44.555
and how might we visualize text?

30
00:01:44.555 --> 00:01:47.820
And in text visualization, basically,

31
00:01:47.820 --> 00:01:53.160
we're trying to think about summarizations of important elements within these documents.

32
00:01:53.160 --> 00:01:57.160
And so, there's lots of different pre-processing steps that we may have to go

33
00:01:57.160 --> 00:02:01.460
through prior to getting information from these text documents.

34
00:02:01.460 --> 00:02:06.740
So again remember, you're given a whole bunch of different documents,

35
00:02:06.740 --> 00:02:09.665
and these can be Tweets,

36
00:02:09.665 --> 00:02:14.785
so 144 characters all the way up to novels thousands of pages.

37
00:02:14.785 --> 00:02:18.275
And remember, we have a bunch of different words.

38
00:02:18.275 --> 00:02:20.180
Each document has a set of words.

39
00:02:20.180 --> 00:02:21.490
We have punctuation.

40
00:02:21.490 --> 00:02:24.770
Even if it's a Tweet, you've still got spaces,

41
00:02:24.770 --> 00:02:27.745
and characters, and information, and those.

42
00:02:27.745 --> 00:02:31.590
We may have people, places, and things.

43
00:02:31.590 --> 00:02:41.890
And so we may want to do what's called named entity recognition to extract people,

44
00:02:41.890 --> 00:02:44.090
places, and things from these different chunks.

45
00:02:44.090 --> 00:02:46.710
We may identify this is a person.

46
00:02:46.710 --> 00:02:47.970
This is a place.

47
00:02:47.970 --> 00:02:50.560
And we can grab all of those words.

48
00:02:50.560 --> 00:02:56.700
And we can even then start thinking about which documents use the same words.

49
00:02:56.700 --> 00:02:59.135
And in text visualization,

50
00:02:59.135 --> 00:03:01.820
oftentimes what we're trying to do is summarize

51
00:03:01.820 --> 00:03:06.390
the most important words and the relationships in the document.

52
00:03:06.390 --> 00:03:09.360
It's not necessarily just about counting the number of

53
00:03:09.360 --> 00:03:16.100
times that XXX appeared in all of these documents,

54
00:03:16.100 --> 00:03:25.115
but it may also be how many times that YYY occurred right next to XXX in a document.

55
00:03:25.115 --> 00:03:27.550
So, there's a lot of analysis we often have to do,

56
00:03:27.550 --> 00:03:33.390
and a lot of process we have to do for text data prior to visualization.

57
00:03:33.390 --> 00:03:35.840
There maybe things like named entity recognition,

58
00:03:35.840 --> 00:03:42.490
maybe things like just counting the number of unique words to get an overview.

59
00:03:42.490 --> 00:03:45.570
It could be things

60
00:03:45.570 --> 00:03:54.045
like Latent Dirichlet allocation to the topic modeling,

61
00:03:54.045 --> 00:03:58.350
so I may want to know what topics are in a particular document.

62
00:03:58.350 --> 00:04:06.080
And one of the most common text visualizations is we just find the most common words,

63
00:04:06.080 --> 00:04:08.920
and we draw the words.

64
00:04:08.920 --> 00:04:11.995
We write the words down, and we create what's called a word cloud.

65
00:04:11.995 --> 00:04:17.610
And so, for example, here's the 2002 State of the Union Address by U.S. President Bush.

66
00:04:17.610 --> 00:04:22.565
And what we did is we counted the frequency of the words in his speech.

67
00:04:22.565 --> 00:04:26.510
So, we're given his speech, and we count how many times he says a particular word.

68
00:04:26.510 --> 00:04:28.380
And notice we get some weird things here,

69
00:04:28.380 --> 00:04:30.630
that we had America and American.

70
00:04:30.630 --> 00:04:32.530
So you can do things like stemming,

71
00:04:32.530 --> 00:04:35.820
and you can try to combine words that are the

72
00:04:35.820 --> 00:04:39.650
same but have slightly different changes to their roots.

73
00:04:39.650 --> 00:04:43.825
Here we see the same problem here with President Obama, American, and America.

74
00:04:43.825 --> 00:04:46.060
So, thinking about how we process the data,

75
00:04:46.060 --> 00:04:50.175
but what this does it allows us to compare and contrast the 2002 speech,

76
00:04:50.175 --> 00:04:57.365
the 2011 speech, and we see that words are highlighted like terrorist and terror pop out.

77
00:04:57.365 --> 00:05:00.240
But here we're seeing things like people,

78
00:05:00.240 --> 00:05:03.945
and new, and work, and dream.

79
00:05:03.945 --> 00:05:06.040
It's not that we're not seeing some of those words here.

80
00:05:06.040 --> 00:05:10.195
We see children and justice.

81
00:05:10.195 --> 00:05:12.950
But we're seeing things like Afghanistan,

82
00:05:12.950 --> 00:05:15.480
so we know that something's going on in Afghanistan during this time.

83
00:05:15.480 --> 00:05:18.460
Here we don't see Afghanistan at all in the word cloud.

84
00:05:18.460 --> 00:05:21.825
And this lets us get an overview about what might be going on.

85
00:05:21.825 --> 00:05:23.840
Compare and contrast two documents and get

86
00:05:23.840 --> 00:05:26.890
a high-level summary of these to allow us to compare.

87
00:05:26.890 --> 00:05:32.335
There's other variations of this called wordles that we can try out,

88
00:05:32.335 --> 00:05:34.735
where wordles do text layout.

89
00:05:34.735 --> 00:05:39.005
Again, changing the size of our text based on count.

90
00:05:39.005 --> 00:05:41.805
But now let's think about other things we get here as well.

91
00:05:41.805 --> 00:05:46.665
We could actually position the words and the position could have meaning.

92
00:05:46.665 --> 00:05:48.625
We could change the color of the words,

93
00:05:48.625 --> 00:05:50.865
and we could change the size of the words.

94
00:05:50.865 --> 00:05:58.910
Now part of the challenge here is America is a relatively long word.

95
00:05:58.910 --> 00:06:01.945
So, it's going to take up more space because of the length.

96
00:06:01.945 --> 00:06:06.785
So, we get challenges in encoding these different variables because

97
00:06:06.785 --> 00:06:12.000
the word length may not have any relationship to a variable,

98
00:06:12.000 --> 00:06:16.990
but the length then gives it strength on the screen.

99
00:06:16.990 --> 00:06:18.190
It attracts our eyes more,

100
00:06:18.190 --> 00:06:20.810
so we have to be careful about these different things.

101
00:06:20.810 --> 00:06:23.740
We often remove small words like a, an,

102
00:06:23.740 --> 00:06:26.680
and the, and we do things that we call stemming.

103
00:06:26.680 --> 00:06:31.360
And there's lots of libraries in Python and the standard for

104
00:06:31.360 --> 00:06:34.990
natural language processing library that help us process these text data

105
00:06:34.990 --> 00:06:38.970
sets to then let us do some visualization and analysis on these.

106
00:06:38.970 --> 00:06:42.735
This is just one example of how we could process text visualization,

107
00:06:42.735 --> 00:06:45.600
and as you take more data mining, machine learning,

108
00:06:45.600 --> 00:06:49.450
you can learn other techniques like Latent Dirichlet allocation for extracting topics.

109
00:06:49.450 --> 00:06:52.715
So we can think about how to show topic changes over time,

110
00:06:52.715 --> 00:06:56.030
and there's tons of really exciting visualizations.

111
00:06:56.030 --> 00:06:57.905
We're trying to summarize documents,

112
00:06:57.905 --> 00:06:59.710
document changes over time,

113
00:06:59.710 --> 00:07:02.730
document relationships between entities,

114
00:07:02.730 --> 00:07:04.520
and all sorts of information.

115
00:07:04.520 --> 00:07:09.255
So, the word cloud is just sort of one quick summary overview.

116
00:07:09.255 --> 00:07:13.355
I don't think anybody would say this is the best visualization necessarily for text,

117
00:07:13.355 --> 00:07:16.330
but this is one of the most common ones you'll see to give you

118
00:07:16.330 --> 00:07:21.030
a quick summary of what's going on in the data. Thank you.