WEBVTT

1
00:00:00.320 --> 00:00:03.580
Welcome back. This is Huan Lui,

2
00:00:03.580 --> 00:00:06.585
professor of computer science and engineering.

3
00:00:06.585 --> 00:00:11.740
In this segment, we are discussing unsupervised learning.

4
00:00:11.740 --> 00:00:19.805
And then we try to give you an idea about what is unsupervised learning,

5
00:00:19.805 --> 00:00:26.580
what is typical method of unsupervised learning?

6
00:00:26.580 --> 00:00:32.915
So, unsupervised learning is also called clustering.

7
00:00:32.915 --> 00:00:41.140
Unsupervised because we don't have labels as in supervised learning.

8
00:00:41.140 --> 00:00:43.805
But we still have data.

9
00:00:43.805 --> 00:00:47.095
And how can we make sense of the data?

10
00:00:47.095 --> 00:00:49.610
Now in this example,

11
00:00:49.610 --> 00:00:51.830
if we look at it,

12
00:00:51.830 --> 00:00:54.795
obviously we see some patterns, right?

13
00:00:54.795 --> 00:00:58.310
And we have blue, red, green,

14
00:00:58.310 --> 00:01:02.210
yellow and they probably mean something.

15
00:01:02.210 --> 00:01:09.430
So, if we can figure out some patterns from the data without labels,

16
00:01:09.430 --> 00:01:15.665
that will be very helpful for us to do various tasks.

17
00:01:15.665 --> 00:01:21.115
So, clustering algorithms group together similar items,

18
00:01:21.115 --> 00:01:27.580
in this case similar color points are clustered together.

19
00:01:27.580 --> 00:01:36.050
Some similar data points together,

20
00:01:36.050 --> 00:01:40.055
so we need a distance measure or similarity measure.

21
00:01:40.055 --> 00:01:49.570
So here you see we we just want to use euclidean distance to illustrate.

22
00:01:49.570 --> 00:01:51.645
Of course there are other methods like

23
00:01:51.645 --> 00:01:58.255
Pearson linear correlation as well others,we will discuss right after this.

24
00:01:58.255 --> 00:02:01.770
So for these two data points,

25
00:02:01.770 --> 00:02:05.645
we can calculate how close they are.

26
00:02:05.645 --> 00:02:08.645
The closer they are,

27
00:02:08.645 --> 00:02:10.600
the more similar they are,

28
00:02:10.600 --> 00:02:14.405
and then we can put them together.

29
00:02:14.405 --> 00:02:26.515
So distance measure is one way to achieve this similarity calculation.

30
00:02:26.515 --> 00:02:34.360
Know for X and Y n-dimensional vectors, right,

31
00:02:34.360 --> 00:02:41.115
so as large one or two,

32
00:02:41.115 --> 00:02:47.605
but N sometimes can be very large in this case you see n can be very large.

33
00:02:47.605 --> 00:02:51.910
Then we can have different measures,

34
00:02:51.910 --> 00:03:03.585
so Marlon Noble's distance is the one we need to calculate the covariance metrics.

35
00:03:03.585 --> 00:03:11.815
In Manhattan distance, it is basically an L1 norm just to calculate the absolute.

36
00:03:11.815 --> 00:03:18.520
difference between two elements of these two vectors.

37
00:03:18.520 --> 00:03:21.470
Then there's the LP norm,

38
00:03:21.470 --> 00:03:27.215
P can be one or two,

39
00:03:27.215 --> 00:03:29.970
or very large number even infinity.

40
00:03:29.970 --> 00:03:39.690
So there are various kinds of distance measures.

41
00:03:40.230 --> 00:03:47.075
Clustering or unsupervised learning,

42
00:03:47.075 --> 00:03:50.985
is to try to find

43
00:03:50.985 --> 00:03:58.305
those clusters within which the instances are similar,

44
00:03:58.305 --> 00:04:04.880
and then how can the weed represent a cluster?

45
00:04:04.880 --> 00:04:08.325
We just use cluster centroids,

46
00:04:08.325 --> 00:04:14.380
so closer centroids can

47
00:04:14.380 --> 00:04:21.860
be way smaller number of cluster centroids way smaller than data points.

48
00:04:21.860 --> 00:04:29.315
So basically we just use these centroids to represent clusters.

49
00:04:29.315 --> 00:04:39.130
So now, one of the most common type of algorithms is partitional method,

50
00:04:39.130 --> 00:04:44.070
you just partition the data into a set of

51
00:04:44.070 --> 00:04:53.000
predefined clusters then you just calculate like each instance in the data set,

52
00:04:53.000 --> 00:04:56.630
see which cluster it is closest to

53
00:04:56.630 --> 00:05:01.095
then assign this instance to that cluster, Okay this is one method.

54
00:05:01.095 --> 00:05:05.435
Of course there's another method then it's bottom up,

55
00:05:05.435 --> 00:05:08.090
but we're not going to discuss this here.

56
00:05:08.090 --> 00:05:12.800
We will focus on the partitional method.

57
00:05:12.800 --> 00:05:16.070
Now in this 2D case,

58
00:05:16.070 --> 00:05:20.165
I have six clusters, right?

59
00:05:20.165 --> 00:05:22.990
So if I know before I have six clusters,

60
00:05:22.990 --> 00:05:27.600
then I will be able to cluster all of them accordingly.

61
00:05:27.600 --> 00:05:32.345
But if I say on a case two,

62
00:05:32.345 --> 00:05:37.015
then probably I have different ways to cluster, right?

63
00:05:37.015 --> 00:05:43.360
It's one here, it's one here or a one here another one here.

64
00:05:43.360 --> 00:05:47.060
So which one's the best?

65
00:05:47.060 --> 00:05:51.190
And there are so many combinations.

66
00:05:51.310 --> 00:05:59.025
We introduce k-means, an intuitive and the common algorithm.

67
00:05:59.025 --> 00:06:01.770
What it does this,

68
00:06:01.770 --> 00:06:08.200
common data points XY in initial set of K centroids.

69
00:06:08.200 --> 00:06:10.440
So, M1 to MK,

70
00:06:10.440 --> 00:06:12.265
this is the one,

71
00:06:12.265 --> 00:06:16.760
and this is T, it's the iteration we have to go through, okay?

72
00:06:16.760 --> 00:06:21.620
So in the very beginning it is one.

73
00:06:21.620 --> 00:06:26.260
We need to have k centroids,

74
00:06:26.260 --> 00:06:30.970
then what do we do is for each data point,

75
00:06:30.970 --> 00:06:38.010
we will compare with all of the centroids and

76
00:06:38.010 --> 00:06:44.570
see if it's close to one of them,

77
00:06:44.570 --> 00:06:53.000
then we can assign this data point to this cluster corresponding to the centroid.

78
00:06:53.000 --> 00:06:57.880
So in this case, you see at the T iteration.

79
00:06:57.880 --> 00:07:01.255
So, if I can find the mi,

80
00:07:01.255 --> 00:07:04.320
this is the centroid i,

81
00:07:04.320 --> 00:07:09.830
close is closest, right?

82
00:07:09.830 --> 00:07:15.160
this is the one it's smaller than all the others, the remaining ones.

83
00:07:15.160 --> 00:07:21.560
Then I will assign this XP to i,

84
00:07:21.560 --> 00:07:26.380
so that's the cluster i,

85
00:07:26.840 --> 00:07:30.995
but this is just like for one,

86
00:07:30.995 --> 00:07:35.595
right, instance, but if I have N instances,

87
00:07:35.595 --> 00:07:40.450
I have to assign all of them accordingly to

88
00:07:40.450 --> 00:07:50.855
these k clusters and that next one should be T equals two now.

89
00:07:50.855 --> 00:07:53.865
But a centroid should be updated,

90
00:07:53.865 --> 00:07:57.665
because now I have each cluster,

91
00:07:57.665 --> 00:08:02.180
I have new instances from the streaming data and I need to

92
00:08:02.180 --> 00:08:07.940
recalculate and then I repeated this process.

93
00:08:07.940 --> 00:08:18.455
This is the algorithm and we have a data set of real value attributes,

94
00:08:18.455 --> 00:08:26.295
K the number of clusters and we will return a clustering.

95
00:08:26.295 --> 00:08:30.840
The result of clustering is called clustering, Okay.

96
00:08:30.840 --> 00:08:34.950
A clustering of data into K clusters.

97
00:08:34.950 --> 00:08:41.595
You say How can I determine the very fastest K centroids?

98
00:08:41.595 --> 00:08:50.665
So this is why we usually we consider key random instances in the data space.

99
00:08:50.665 --> 00:08:54.860
As the initial cluster centroids.

100
00:08:54.860 --> 00:08:58.310
Now, after we determine that,

101
00:08:58.310 --> 00:09:01.300
we will go through the whole dataset.

102
00:09:01.300 --> 00:09:05.110
While centroids have not converged.

103
00:09:05.110 --> 00:09:07.940
What does that mean? We will do this later.

104
00:09:07.940 --> 00:09:13.035
So, if it's not defined, this convergence later.

105
00:09:13.035 --> 00:09:19.000
So, assign each instance to the cluster that has the closest cluster centroid.

106
00:09:19.000 --> 00:09:21.825
If all instances have been assigned,

107
00:09:21.825 --> 00:09:29.105
then recalculate the cluster centroid by averaging instances inside each cluster.

108
00:09:29.105 --> 00:09:35.705
So it's very intuitive. So that's why for any clustering work,

109
00:09:35.705 --> 00:09:38.290
we need to use k-means.

110
00:09:38.290 --> 00:09:43.485
If we know k, then we use k-Means as a baseline algorithm,

111
00:09:43.485 --> 00:09:48.970
to compare with other more complicated algorithms.

112
00:09:48.970 --> 00:09:58.710
This is intuitive that's why people tend to accept the result more.

113
00:09:58.710 --> 00:10:00.210
But the issue is,

114
00:10:00.210 --> 00:10:04.425
how can I determine the right k, right?

115
00:10:04.425 --> 00:10:07.600
So there are certain issues,

116
00:10:07.600 --> 00:10:12.610
and when do we stop?

117
00:10:12.610 --> 00:10:15.255
So we mentioned about the convergence.

118
00:10:15.255 --> 00:10:23.865
So, convergence can be defined when centroids are no longer changing.

119
00:10:23.865 --> 00:10:28.990
So, like at t iteration,

120
00:10:28.990 --> 00:10:31.880
I have k centroids.

121
00:10:31.880 --> 00:10:34.660
At t plus one iterations,

122
00:10:34.660 --> 00:10:40.520
the set of centroids is the same as the one before,

123
00:10:40.520 --> 00:10:46.985
the set before, then we say it convergences.

124
00:10:46.985 --> 00:10:53.255
But there's another way to define convergence.

125
00:10:53.255 --> 00:10:58.770
We can't just say algorithms can be stopped when the Euclidean distance between

126
00:10:58.770 --> 00:11:05.660
centroids in two consecutive steps is less than some small positive value.

127
00:11:05.660 --> 00:11:13.410
Why? Because k-means does not guarantee that the algorithm will converge.

128
00:11:13.410 --> 00:11:15.425
So in this case,

129
00:11:15.425 --> 00:11:18.550
of course, we cannot wait forever.

130
00:11:18.550 --> 00:11:23.815
So we have to use this distance,

131
00:11:23.815 --> 00:11:29.865
like a small distance between two sets of centroids to decide when to stop.

132
00:11:29.865 --> 00:11:35.800
So an alternative to k-means is to

133
00:11:35.800 --> 00:11:42.955
implement it as an objective function then we try to minimize it.

134
00:11:42.955 --> 00:11:44.895
So, what is that?

135
00:11:44.895 --> 00:11:49.270
Again, we tried to calculate

136
00:11:49.270 --> 00:11:57.680
the distance between data points in this cluster, ith cluster.

137
00:11:57.680 --> 00:12:00.160
You see, we have k clusters.

138
00:12:00.160 --> 00:12:05.260
I is one to k. So,

139
00:12:05.260 --> 00:12:09.720
for ith cluster, so n right?

140
00:12:09.720 --> 00:12:18.630
I have n_i means I have how many instances in this ith cluster?

141
00:12:18.630 --> 00:12:25.050
So I just calculated the distance between each instance in

142
00:12:25.050 --> 00:12:33.575
this cluster with the centroid then square it.

143
00:12:33.575 --> 00:12:37.995
Why? Because the distance must be positive,

144
00:12:37.995 --> 00:12:44.870
but x_ji minus c_i can be positive or negative.

145
00:12:44.870 --> 00:12:47.145
So that's why we have to square it.

146
00:12:47.145 --> 00:12:49.815
Then we add them together.

147
00:12:49.815 --> 00:12:51.725
We calculate the total error.

148
00:12:51.725 --> 00:12:54.115
But now we want to minimize it,

149
00:12:54.115 --> 00:12:55.385
then this is a good one.

150
00:12:55.385 --> 00:13:00.770
Okay? So stopping criteria here is when the difference between

151
00:13:00.770 --> 00:13:04.470
objective function values of two consecutive iterations

152
00:13:04.470 --> 00:13:09.115
of k-means algorithms is less than some small value.

153
00:13:09.115 --> 00:13:15.310
Then we stop. Now, more discussion.

154
00:13:15.310 --> 00:13:24.170
So, one is how can we know we have the right k? What should we do?

155
00:13:24.170 --> 00:13:31.765
We can always talk to domain experts if they have a sense of the data,

156
00:13:31.765 --> 00:13:40.130
or if they want you to cluster data into certain number of k clusters.

157
00:13:40.130 --> 00:13:45.895
But another way is your let your algorithm tell you.

158
00:13:45.895 --> 00:13:51.100
So you just ran like a k equals three,

159
00:13:51.100 --> 00:13:55.380
five, seven, whatever number,

160
00:13:55.380 --> 00:13:58.245
just let the computer work for you.

161
00:13:58.245 --> 00:14:05.180
But finding global optimum of k partition is computationally expensive.

162
00:14:05.180 --> 00:14:07.545
It's difficult to programme.

163
00:14:07.545 --> 00:14:14.840
And this is equivalent to finding optimal centroids that minimize objective function,

164
00:14:14.840 --> 00:14:18.770
since we cannot wait forever.

165
00:14:18.770 --> 00:14:26.975
So we just try to use some heuristics to help us.

166
00:14:26.975 --> 00:14:30.570
Because at the end of the day when we do clustering,

167
00:14:30.570 --> 00:14:32.370
we want to understand the data.

168
00:14:32.370 --> 00:14:35.950
Then, after we gain some understanding,

169
00:14:35.950 --> 00:14:41.565
we can always revise our hypothesis and retry.

170
00:14:41.565 --> 00:14:50.415
Okay? That's why we can run k-means multiple times.

171
00:14:50.415 --> 00:14:56.305
So, run k-means multiple times has several meanings.

172
00:14:56.305 --> 00:15:01.795
One is I can change k. The other one is after I fix a k value,

173
00:15:01.795 --> 00:15:03.965
I can also run multiple times.

174
00:15:03.965 --> 00:15:09.520
Why? Because remember in the very beginning for us to choose k centroids,

175
00:15:09.520 --> 00:15:13.970
it's just a random process.

176
00:15:13.970 --> 00:15:19.225
So we never know which random set is the best.

177
00:15:19.225 --> 00:15:22.885
So, we can just run several times,

178
00:15:22.885 --> 00:15:26.820
and then we will be able to find the good one.

179
00:15:26.820 --> 00:15:30.860
But now, we also want to talk about

180
00:15:30.860 --> 00:15:37.090
the difference between means and medians, okay? So why?

181
00:15:37.090 --> 00:15:45.105
Because, mean we know is the average,

182
00:15:45.105 --> 00:15:48.130
and median is the middle value.

183
00:15:48.130 --> 00:15:50.235
There are differences.

184
00:15:50.235 --> 00:15:54.505
Okay?. So, here is an example.

185
00:15:54.505 --> 00:15:57.620
We have a startup with eight employees,

186
00:15:57.620 --> 00:15:59.560
one CTO and one CEO.

187
00:15:59.560 --> 00:16:03.740
These are their salaries: 50k,

188
00:16:03.740 --> 00:16:06.600
50k, 50k, 50k, 50k.

189
00:16:06.600 --> 00:16:09.360
Five employees with 50k,

190
00:16:09.360 --> 00:16:12.920
and then two employees with 80k,

191
00:16:12.920 --> 00:16:18.815
and one employee has 90k,

192
00:16:18.815 --> 00:16:23.200
and the CTO and CEO each has 150K.

193
00:16:23.200 --> 00:16:25.315
So what's the average?

194
00:16:25.315 --> 00:16:32.580
The average of the salary is if you calculate,

195
00:16:32.580 --> 00:16:36.680
like you add them all up and divide by 10, right?

196
00:16:36.680 --> 00:16:46.240
So, it is 80k. What is the median?

197
00:16:48.420 --> 00:16:53.580
Medium is the middle value.

198
00:16:53.580 --> 00:16:55.240
Now, I have 10 values.

199
00:16:55.240 --> 00:16:58.575
So, one, two, three, four, five,

200
00:16:58.575 --> 00:17:02.220
this is the one, six this is the one.

201
00:17:02.220 --> 00:17:03.830
Because it's even number,

202
00:17:03.830 --> 00:17:06.495
I have to add these two up and divide by two.

203
00:17:06.495 --> 00:17:11.045
So it's 65k. Do you see?

204
00:17:11.045 --> 00:17:15.895
Which one represents this group of employees,

205
00:17:15.895 --> 00:17:19.850
this company's salary better?

206
00:17:19.850 --> 00:17:23.510
It seems 65k is more reasonable, right?

207
00:17:23.510 --> 00:17:31.165
But if we change let's say CEO's salary to one million,

208
00:17:31.165 --> 00:17:38.850
and the other employees' salaries remain the same,

209
00:17:38.850 --> 00:17:43.015
now, we have an average.

210
00:17:43.015 --> 00:17:52.500
You just add all these up 50k times five,

211
00:17:52.500 --> 00:17:58.570
80k times two plus 90k

212
00:17:58.570 --> 00:18:05.655
plus 150k plus one million divided by 10.

213
00:18:05.655 --> 00:18:11.640
It is 165k.

214
00:18:11.640 --> 00:18:17.280
Now suddenly this company's average salary is 165k.

215
00:18:17.280 --> 00:18:18.865
Of course, it's outrageous.

216
00:18:18.865 --> 00:18:20.110
It's not real at all.

217
00:18:20.110 --> 00:18:25.225
Right? Even the CTO does not acquire such a high salary.

218
00:18:25.225 --> 00:18:30.205
Okay? So that's why average can be misleading.

219
00:18:30.205 --> 00:18:32.595
But now what is the median?

220
00:18:32.595 --> 00:18:34.835
The median remains the same,

221
00:18:34.835 --> 00:18:39.715
still 65k because it's still these two values, the middle value.

222
00:18:39.715 --> 00:18:43.390
So, should we use mean or medians?

223
00:18:43.390 --> 00:18:48.800
Right.? It depends. If I have a large set of data,

224
00:18:48.800 --> 00:18:51.975
then which one is easier to update?

225
00:18:51.975 --> 00:18:53.350
For the k-means right?

226
00:18:53.350 --> 00:18:57.445
We have to update the means. So you see.

227
00:18:57.445 --> 00:19:04.745
But if we, and sometimes we don't want to update too frequently,

228
00:19:04.745 --> 00:19:09.755
then the average seems is the value you can easy to update.

229
00:19:09.755 --> 00:19:14.155
Median, it is not because if you add one person,

230
00:19:14.155 --> 00:19:18.190
let's say if you add another 50k here,

231
00:19:18.190 --> 00:19:21.880
this will be the middle of value, right?

232
00:19:21.880 --> 00:19:25.410
So, that means it can change,

233
00:19:25.410 --> 00:19:28.180
that means you have to recalculate.

234
00:19:28.180 --> 00:19:32.340
That's the end of unsupervised learning.

235
00:19:32.340 --> 00:19:38.000
We just discussed one intuitive algorithm. It's called the k-means.