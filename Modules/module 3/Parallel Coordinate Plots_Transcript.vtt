WEBVTT

1
00:00:00.000 --> 00:00:04.090
In this module, we want to talk about a unique sort

2
00:00:04.090 --> 00:00:09.975
of visualization space called parallel coordinate plots.

3
00:00:09.975 --> 00:00:14.485
We're going to look at different attributes of this multivariate data visualization.

4
00:00:14.485 --> 00:00:21.160
So much like the scatter plots and the other multivariate data visualization lectures,

5
00:00:21.160 --> 00:00:23.425
again we're thinking about a dataset.

6
00:00:23.425 --> 00:00:26.070
For example we could have our players again,

7
00:00:26.070 --> 00:00:27.630
could a batting average,

8
00:00:27.630 --> 00:00:29.650
we could have percent on base,

9
00:00:29.650 --> 00:00:31.030
we could have countries,

10
00:00:31.030 --> 00:00:32.535
we could have income,

11
00:00:32.535 --> 00:00:36.785
GDP, population, and so forth.

12
00:00:36.785 --> 00:00:39.420
So we've got data sets with lots of variables,

13
00:00:39.420 --> 00:00:43.580
lots of rows, lots of information,

14
00:00:43.580 --> 00:00:47.700
and we want to think about how to create a plot

15
00:00:47.700 --> 00:00:52.360
that lets people explore trends and relationships between those.

16
00:00:52.360 --> 00:00:55.990
And in previous modules we talked about the scatter plot where we can

17
00:00:55.990 --> 00:01:02.290
compare things like population size and GDP for example,

18
00:01:02.290 --> 00:01:05.685
and each dot might be a particular country.

19
00:01:05.685 --> 00:01:08.860
We might see some sort of trends.

20
00:01:08.860 --> 00:01:12.470
We can extend those by changing size and shape

21
00:01:12.470 --> 00:01:15.990
and color and adding more variables into our data sets.

22
00:01:15.990 --> 00:01:18.115
And in parallel coordinate plots,

23
00:01:18.115 --> 00:01:20.290
we have a similar sort of idea,

24
00:01:20.290 --> 00:01:25.805
where now different variables can take different values with different ranges.

25
00:01:25.805 --> 00:01:32.905
And what's going to happen is if we have a data set for example like country,

26
00:01:32.905 --> 00:01:42.945
GDP, population, let's have some sort of measure like wellness,

27
00:01:42.945 --> 00:01:48.755
maybe it's the average age people live life expectancy and things like that.

28
00:01:48.755 --> 00:01:50.900
Let's think of some other variables.

29
00:01:50.900 --> 00:01:54.520
We will just call them variable one variable two and variable three.

30
00:01:54.520 --> 00:01:56.850
So you can have a ton of variables about a country.

31
00:01:56.850 --> 00:01:59.940
We could have size for example,

32
00:01:59.940 --> 00:02:02.605
what's the landmass of the country,

33
00:02:02.605 --> 00:02:07.700
number of provinces, or states or whatever in a country and so forth.

34
00:02:07.700 --> 00:02:09.360
So we have all these different measures.

35
00:02:09.360 --> 00:02:13.120
Well, for each measure, we have an axes.

36
00:02:13.120 --> 00:02:15.240
So I've got population,

37
00:02:15.240 --> 00:02:21.015
and maybe population ranges from zero to one and a half billion.

38
00:02:21.015 --> 00:02:25.780
Now there's only a few countries that have one and a half billion

39
00:02:25.780 --> 00:02:29.960
and there's a bunch of countries that maybe have several millions here,

40
00:02:29.960 --> 00:02:32.200
so we wind up with this skewed distribution.

41
00:02:32.200 --> 00:02:37.560
Now the other weird thing is that population goes from zero to one and a half billion.

42
00:02:37.560 --> 00:02:41.485
But if we do life expectancy,

43
00:02:41.485 --> 00:02:46.025
it's sort of maybe goes from zero to 100.

44
00:02:46.025 --> 00:02:50.635
So how do I match 100 to one and a half billion?

45
00:02:50.635 --> 00:02:53.825
These axes have such drastically different values,

46
00:02:53.825 --> 00:02:58.290
so oftentimes we might try to normalize these from zero to one.

47
00:02:58.290 --> 00:03:01.570
What I mean by that is I might find the maximum and I might

48
00:03:01.570 --> 00:03:06.830
divide everything by the max in this column to try and normalize the data.

49
00:03:06.830 --> 00:03:19.150
And now what you're starting to see when I draw my graph like this,

50
00:03:19.150 --> 00:03:23.220
is for each variable, I can create a line.

51
00:03:23.220 --> 00:03:26.115
So for GDP, I can create a line.

52
00:03:26.115 --> 00:03:28.845
For population, I can create a line.

53
00:03:28.845 --> 00:03:31.665
For life expectancy, I can create a line.

54
00:03:31.665 --> 00:03:33.785
For variable one, I create a line.

55
00:03:33.785 --> 00:03:35.435
And the more variables we get,

56
00:03:35.435 --> 00:03:37.470
the more these axes we're going to have.

57
00:03:37.470 --> 00:03:41.485
So these are my parallel coordinate axes.

58
00:03:41.485 --> 00:03:45.140
Now, I want to plot countries for example.

59
00:03:45.140 --> 00:03:48.455
So for every unique country,

60
00:03:48.455 --> 00:03:50.760
the unique country has a GDP.

61
00:03:50.760 --> 00:03:54.830
And remember I can just do the 1DK,

62
00:03:54.830 --> 00:03:58.765
so every dot on this line is already a country's GDP,

63
00:03:58.765 --> 00:04:02.825
and every dot on this line is a country's population.

64
00:04:02.825 --> 00:04:11.525
Parallel coordinate plots connect the same countries together across each of these lines.

65
00:04:11.525 --> 00:04:15.140
So this line now represents a single country,

66
00:04:15.140 --> 00:04:19.385
and where it crosses is the values in our data table.

67
00:04:19.385 --> 00:04:21.940
So we can figure out what that country's GDP is,

68
00:04:21.940 --> 00:04:25.810
we can figure out what its population is and so forth tracking those.

69
00:04:25.810 --> 00:04:32.135
And a parallel coordinate plot has a line for every country.

70
00:04:32.135 --> 00:04:34.070
And again, what we're seeing is

71
00:04:34.070 --> 00:04:38.580
these pairwise combinations on a parallel coordinate plot.

72
00:04:38.580 --> 00:04:44.740
And so we're just putting all of our variables on different axes and

73
00:04:44.740 --> 00:04:50.610
then we can connect based on our categorical variable interest or things like that.

74
00:04:50.610 --> 00:04:54.130
So if this is eight quizzes in class,

75
00:04:54.130 --> 00:04:57.785
each line could be a student representing that student's record.

76
00:04:57.785 --> 00:05:00.545
Now the thing is with parallel coordinate plots,

77
00:05:00.545 --> 00:05:03.995
I didn't have to put V8 next to V7.

78
00:05:03.995 --> 00:05:09.410
I could have moved V8 over to V4 and V4 back to V8.

79
00:05:09.410 --> 00:05:12.830
What happens in a parallel coordinate plot is even though I can now

80
00:05:12.830 --> 00:05:16.505
see all of the data records in this single view,

81
00:05:16.505 --> 00:05:19.715
I can only see sort of pairwise correlations.

82
00:05:19.715 --> 00:05:25.385
So here I can see most of the data from V1 to V2 has a downward trend.

83
00:05:25.385 --> 00:05:28.345
Meaning that in general, V1,

84
00:05:28.345 --> 00:05:31.320
if I do have a high value in V1,

85
00:05:31.320 --> 00:05:33.830
I generally have a lower value in V2.

86
00:05:33.830 --> 00:05:36.670
If I look at V4 to V5,

87
00:05:36.670 --> 00:05:38.090
I have a lower volume V4,

88
00:05:38.090 --> 00:05:40.380
I have a higher value in V5.

89
00:05:40.380 --> 00:05:44.510
So these trends let me still look at pairwise comparison and it

90
00:05:44.510 --> 00:05:48.590
can even show V3 to V4 and V4 to V5.

91
00:05:48.590 --> 00:05:51.085
So I can sort of look at two correlations at once,

92
00:05:51.085 --> 00:05:52.855
I can reorder things,

93
00:05:52.855 --> 00:05:55.570
but the order of these axes is going to greatly

94
00:05:55.570 --> 00:05:59.345
influence what this visualization looks like.

95
00:05:59.345 --> 00:06:05.080
Likewise, what's really important actually is the angle between the axes,

96
00:06:05.080 --> 00:06:09.790
because these angles represent this level of correlation,

97
00:06:09.790 --> 00:06:12.625
and we keep coming back and talking about correlation.

98
00:06:12.625 --> 00:06:15.610
The reason why correlation is important is because it

99
00:06:15.610 --> 00:06:18.330
allows me to create some sort of mathematical formula,

100
00:06:18.330 --> 00:06:21.960
like X is equal to MY.

101
00:06:21.960 --> 00:06:24.170
And if I can make some formula like this,

102
00:06:24.170 --> 00:06:26.590
if I know Y,

103
00:06:26.590 --> 00:06:28.495
I can predict X.

104
00:06:28.495 --> 00:06:31.695
And if I can predict X, if I know something about the future,

105
00:06:31.695 --> 00:06:34.750
if I know something about other things I haven't seen,

106
00:06:34.750 --> 00:06:36.440
so for example if I'm trying to hire

107
00:06:36.440 --> 00:06:39.750
new baseball player and I want to guess how he might do and I don't have

108
00:06:39.750 --> 00:06:42.230
any measurements on their batting average

109
00:06:42.230 --> 00:06:44.650
but I have lots of measurements on their on base percent,

110
00:06:44.650 --> 00:06:47.680
I can guess they're batting average from this information.

111
00:06:47.680 --> 00:06:51.100
For stock markets, if I can make some sort of

112
00:06:51.100 --> 00:06:55.050
equation like this where I know how much stock Y has made,

113
00:06:55.050 --> 00:06:57.100
I can guess how much stock X will make

114
00:06:57.100 --> 00:06:59.680
and I might be able to predict things in the future,

115
00:06:59.680 --> 00:07:03.070
I might be able to classify unknown information.

116
00:07:03.070 --> 00:07:07.340
So that's why looking and exploring data is really important,

117
00:07:07.340 --> 00:07:09.530
and parallel co-ordinate plots lets us

118
00:07:09.530 --> 00:07:12.960
see different correlations between subsets of the data.

119
00:07:12.960 --> 00:07:15.800
So we can even start thinking about is there a subset of the data,

120
00:07:15.800 --> 00:07:20.080
like this subset here that is correlated and why is that

121
00:07:20.080 --> 00:07:25.100
subset correlated where other subsets are not.

122
00:07:25.100 --> 00:07:28.530
So we can start reasoning and exploring different chunks and we can see

123
00:07:28.530 --> 00:07:32.775
which correlations along two axes are of interest.

124
00:07:32.775 --> 00:07:34.810
And again here we're looking at a car data sets.

125
00:07:34.810 --> 00:07:36.465
We have the year car was made,

126
00:07:36.465 --> 00:07:39.085
it's horsepower, it's acceleration,

127
00:07:39.085 --> 00:07:41.050
the number of cylinders.

128
00:07:41.050 --> 00:07:45.395
So we can start exploring and extracting information.

129
00:07:45.395 --> 00:07:51.465
And what's interesting is we can even think about looking at how these elements cluster,

130
00:07:51.465 --> 00:07:57.050
and we can see the visual clustering of the data in the parallel coordinate plot,

131
00:07:57.050 --> 00:08:00.750
and so we can apply color and opacity based on line density.

132
00:08:00.750 --> 00:08:04.400
So the more elements across a particular chunk,

133
00:08:04.400 --> 00:08:05.780
the more dense they can get.

134
00:08:05.780 --> 00:08:10.205
We can compute local density for each line and average of the density values,

135
00:08:10.205 --> 00:08:13.510
and we can apply color and opacity based on user specifications.

136
00:08:13.510 --> 00:08:16.475
So we can start filtering things out, looking for trends.

137
00:08:16.475 --> 00:08:18.280
We don't even have to draw straight lines.

138
00:08:18.280 --> 00:08:20.430
As you see here, we can try to curve things

139
00:08:20.430 --> 00:08:25.810
to show different of patterns and get more different visual aesthetics,

140
00:08:25.810 --> 00:08:29.185
because one of the big challenges with parallel coordinate plots is,

141
00:08:29.185 --> 00:08:32.890
if I have a really large set of

142
00:08:32.890 --> 00:08:38.345
lines and a really large set of countries or baseball players,

143
00:08:38.345 --> 00:08:43.900
I can wind up with plots like this where it's just really hard to see anything,

144
00:08:43.900 --> 00:08:45.610
I have too many lines that overlap.

145
00:08:45.610 --> 00:08:48.920
So we started thinking about how we might be able to bundle these together.

146
00:08:48.920 --> 00:08:51.630
How can we use color and opacity to help show trends?

147
00:08:51.630 --> 00:08:55.470
How can we allow the user to select different things that are important?

148
00:08:55.470 --> 00:08:58.895
They may say well, I'm only really interested in countries with

149
00:08:58.895 --> 00:09:03.950
a low GDP and a high life expectancy,

150
00:09:03.950 --> 00:09:05.790
because those things are sort of interesting.

151
00:09:05.790 --> 00:09:07.730
I wonder why that might occur.

152
00:09:07.730 --> 00:09:13.265
And so again we can allow user interaction to filter to give information on the tooltip,

153
00:09:13.265 --> 00:09:16.995
to even reorganize axes in the parallel coordinate plot.

154
00:09:16.995 --> 00:09:21.585
And just like we talked about scatter plots,

155
00:09:21.585 --> 00:09:26.935
we can look at screens based metrics to calculate insight into these plots.

156
00:09:26.935 --> 00:09:33.995
In scatter plots we talked about things like skewness, clumpiness.

157
00:09:33.995 --> 00:09:38.350
Striations, and with scattered plots.

158
00:09:38.350 --> 00:09:41.395
Once we draw the plot,

159
00:09:41.395 --> 00:09:45.290
we're trying to measure what this geometry looks like,

160
00:09:45.290 --> 00:09:47.025
and the way the geometry looks,

161
00:09:47.025 --> 00:09:50.905
may give us insight into whether this plot is interesting for humans to look at.

162
00:09:50.905 --> 00:09:52.325
By that same token,

163
00:09:52.325 --> 00:09:55.550
with parallel coordinate plots,

164
00:09:55.550 --> 00:09:59.545
once we draw our geometry,

165
00:09:59.545 --> 00:10:03.875
we can start doing some sort of Screen Space Metrics

166
00:10:03.875 --> 00:10:08.465
to try to understand what this might look like.

167
00:10:08.465 --> 00:10:11.820
Likewise, we can also think about how we

168
00:10:11.820 --> 00:10:14.695
could create lower dimensional projections of the data.

169
00:10:14.695 --> 00:10:17.915
So, taking all of these N variables,

170
00:10:17.915 --> 00:10:21.370
and reducing them to just the two or three most important variables,

171
00:10:21.370 --> 00:10:26.005
so that we only have to plot the ones that would give the maximum insight into the data.

172
00:10:26.005 --> 00:10:28.440
This would help us optimize the parameter space

173
00:10:28.440 --> 00:10:32.490
for things like pixel based orientations and visualizations.

174
00:10:32.490 --> 00:10:36.950
And we can have metrics then based on particular views of the parallel coordinate plots.

175
00:10:36.950 --> 00:10:40.300
Problem is this also depends on the size of the display,

176
00:10:40.300 --> 00:10:44.275
and the space between the axes is where interesting patterns occur.

177
00:10:44.275 --> 00:10:46.480
But the more variables I have,

178
00:10:46.480 --> 00:10:48.840
the more axes I have to draw.

179
00:10:48.840 --> 00:10:52.460
And sometimes then if I have a lot of axes I have to draw,

180
00:10:52.460 --> 00:10:54.825
I may not have a lot of space.

181
00:10:54.825 --> 00:10:57.600
Likewise, this gets really long,

182
00:10:57.600 --> 00:11:00.980
but I have a lot of screen space,

183
00:11:00.980 --> 00:11:03.895
I might have screen space appear that I don't even wind up using.

184
00:11:03.895 --> 00:11:06.140
I can take this and I can rotate it,

185
00:11:06.140 --> 00:11:09.785
so I don't have to draw my parallel axes vertically.

186
00:11:09.785 --> 00:11:12.700
I can do my parallel coordinate plots this direction,

187
00:11:12.700 --> 00:11:14.245
and I can draw my lines as well.

188
00:11:14.245 --> 00:11:17.450
But now I'm losing out on this green space over here.

189
00:11:17.450 --> 00:11:20.880
So again, thinking about trade offs between

190
00:11:20.880 --> 00:11:24.640
these different visualizations of scatter plots and parallel coordinate plots.

191
00:11:24.640 --> 00:11:27.425
And with parallel coordinate plots, basically again,

192
00:11:27.425 --> 00:11:30.140
each connection between an axis,

193
00:11:30.140 --> 00:11:33.305
gives us some information about the data.

194
00:11:33.305 --> 00:11:39.860
Well, we can use a variety of metrics to try and optimize the use of screen space.

195
00:11:39.860 --> 00:11:42.440
For example, we can look at histogram distance,

196
00:11:42.440 --> 00:11:45.400
like recording the slope of the lines between the axes.

197
00:11:45.400 --> 00:11:48.440
We can use paired histograms or histogram

198
00:11:48.440 --> 00:11:51.575
of all the lines covering both axes to try to determine,

199
00:11:51.575 --> 00:11:54.425
you know, should I put these two axes further apart,

200
00:11:54.425 --> 00:11:56.545
and these two axes closer together?

201
00:11:56.545 --> 00:11:58.130
What sort of information is there?

202
00:11:58.130 --> 00:12:01.495
Can I delete this axis altogether because it's not interesting?

203
00:12:01.495 --> 00:12:04.620
So, we can start using these different metrics

204
00:12:04.620 --> 00:12:10.185
and information to again sort of analyze the data first,

205
00:12:10.185 --> 00:12:14.700
present what's interesting, let the user filter, analyze again,

206
00:12:14.700 --> 00:12:17.195
and try to help them explore

207
00:12:17.195 --> 00:12:21.595
form hypotheses and understand what's going on within the data set.

208
00:12:21.595 --> 00:12:24.460
And again there's a variety of metrics to optimize the use of

209
00:12:24.460 --> 00:12:26.770
screen space like line crossing.

210
00:12:26.770 --> 00:12:31.125
So we can interpret each line between a pair of axes as a directed interval,

211
00:12:31.125 --> 00:12:34.835
and sort of count the number of times that the lines cross.

212
00:12:34.835 --> 00:12:36.770
And again, think about it this way.

213
00:12:36.770 --> 00:12:40.485
If we have no line crossings between two axes,

214
00:12:40.485 --> 00:12:45.095
this means that every time a value goes from low to high,

215
00:12:45.095 --> 00:12:47.105
or could have been from high to low,

216
00:12:47.105 --> 00:12:49.875
we easily can sort of sense this pattern.

217
00:12:49.875 --> 00:12:53.960
We can also have the angles of crossing to determine angles between line crossing.

218
00:12:53.960 --> 00:12:58.650
So if I have two axes and I'm getting all of these crossed lines,

219
00:12:58.650 --> 00:13:03.455
I can measure the angle between each cross pair,

220
00:13:03.455 --> 00:13:06.390
and use that as some sort of metric as well.

221
00:13:06.390 --> 00:13:10.530
And we don't have to do this only for quantitative data,

222
00:13:10.530 --> 00:13:13.590
we can actually do this for sets of data as well.

223
00:13:13.590 --> 00:13:17.980
And so, Robert Kosara introduced this idea of parallel sets,

224
00:13:17.980 --> 00:13:19.385
where we can adopt

225
00:13:19.385 --> 00:13:23.430
this parallel coordinate layout by use a frequency based interpretation.

226
00:13:23.430 --> 00:13:27.060
So for example, there's a nice data set about the Titanic.

227
00:13:27.060 --> 00:13:35.605
So, how many male passengers were in first class and survived the Titanic,

228
00:13:35.605 --> 00:13:39.390
versus second class, or third class, or so on.

229
00:13:39.390 --> 00:13:44.980
So our data set winds up looking something like this.

230
00:13:44.980 --> 00:13:50.800
So, we have first class,

231
00:13:50.800 --> 00:13:53.960
second class, third class.

232
00:13:53.960 --> 00:13:56.830
We have male, we have female,

233
00:13:56.830 --> 00:14:03.820
we may also have some sort of a secondary role like survived not survived.

234
00:14:10.840 --> 00:14:15.770
And so, I might know how many survivors were in first class,

235
00:14:15.770 --> 00:14:17.190
maybe this was 30,

236
00:14:17.190 --> 00:14:19.560
and maybe not survived was one,

237
00:14:19.560 --> 00:14:26.995
and maybe this was 25,

238
00:14:26.995 --> 00:14:29.120
and not survived was zero.

239
00:14:29.120 --> 00:14:30.720
But then by the time we get the third class,

240
00:14:30.720 --> 00:14:32.825
it may have been 300,

241
00:14:32.825 --> 00:14:35.640
and 279 did not survive,

242
00:14:35.640 --> 00:14:41.320
and something like 300 and 100 didn't survive.

243
00:14:41.320 --> 00:14:43.360
So again we have this sort of data set,

244
00:14:43.360 --> 00:14:46.855
and we have this sort of frequency based representation.

245
00:14:46.855 --> 00:14:54.890
So, the width of our line here represents the number of males that were in first class.

246
00:14:54.890 --> 00:14:59.010
This bar sort of shows us how many males were on the boat, and how many females.

247
00:14:59.010 --> 00:15:02.480
We split this axis into its overall count.

248
00:15:02.480 --> 00:15:05.190
So I can count the number of men and women on the boat by

249
00:15:05.190 --> 00:15:08.540
adding up all of these different boxes.

250
00:15:08.540 --> 00:15:12.360
And so the length of this part of the axis is the number of males,

251
00:15:12.360 --> 00:15:14.090
length here's the number of females,

252
00:15:14.090 --> 00:15:16.350
the length here is the number of people that didn't survive,

253
00:15:16.350 --> 00:15:17.800
the length here is the number of people that did

254
00:15:17.800 --> 00:15:21.305
survive and the length here is the number people in first class,

255
00:15:21.305 --> 00:15:23.670
second and third class, and then crew.

256
00:15:23.670 --> 00:15:26.015
So I can quickly get an overview of

257
00:15:26.015 --> 00:15:29.800
first class had many less passengers than second and third added together,

258
00:15:29.800 --> 00:15:33.325
and the crew had a similar size the second and third class.

259
00:15:33.325 --> 00:15:37.525
I can look and see how many people in first class were male,

260
00:15:37.525 --> 00:15:40.910
how many people in first class were female, and how many survived.

261
00:15:40.910 --> 00:15:43.885
And we can see actually all the females in first class,

262
00:15:43.885 --> 00:15:49.160
none of them had a green line to not survive.

263
00:15:49.160 --> 00:15:51.445
So, all the women in first class survived,

264
00:15:51.445 --> 00:15:54.455
only a small chunk of men in first class didn't survive.

265
00:15:54.455 --> 00:15:55.970
But when we get to second class,

266
00:15:55.970 --> 00:15:58.740
we see a large chunk of men who were in second and third class,

267
00:15:58.740 --> 00:16:00.980
and the survival rate was really low.

268
00:16:00.980 --> 00:16:02.340
And even for females,

269
00:16:02.340 --> 00:16:04.310
even though we had a pretty decent survival rate,

270
00:16:04.310 --> 00:16:07.875
we have a much higher death rate than we had in first class.

271
00:16:07.875 --> 00:16:10.650
And this parallel set lets us sort of compare

272
00:16:10.650 --> 00:16:13.200
and reason with these data sets to see how these sets of

273
00:16:13.200 --> 00:16:19.530
data overlap and float to their different variables.

274
00:16:19.530 --> 00:16:24.990
And there's been things on how we might update parallel sets with other variables,

275
00:16:24.990 --> 00:16:27.820
so looking at distributions along axes.

276
00:16:27.820 --> 00:16:32.780
So, one of the parallel axes could be for family data set.

277
00:16:32.780 --> 00:16:35.770
So, market, family type income,

278
00:16:35.770 --> 00:16:37.990
and looking like unemployment distributions for

279
00:16:37.990 --> 00:16:40.405
different classes of people on a different axis.

280
00:16:40.405 --> 00:16:42.530
So, there's all sorts of ways we can try to think

281
00:16:42.530 --> 00:16:45.105
about how we can enhance and interpret this data.

282
00:16:45.105 --> 00:16:50.005
And we don't have to lay this out in a horizontal or vertical pattern,

283
00:16:50.005 --> 00:16:55.725
we can even think about laying out these axes in a circular pattern,

284
00:16:55.725 --> 00:16:57.575
to create what we call star plots.

285
00:16:57.575 --> 00:17:01.055
So again now each axis is still next to a neighbor.

286
00:17:01.055 --> 00:17:03.990
So this is our baseball example,

287
00:17:03.990 --> 00:17:05.795
so we might have data about At bats,

288
00:17:05.795 --> 00:17:08.060
Runs, RBI, Batting Average.

289
00:17:08.060 --> 00:17:09.910
And for each player,

290
00:17:09.910 --> 00:17:12.240
I can again draw my line.

291
00:17:12.240 --> 00:17:17.240
And I could put all the players on one plot if I want.

292
00:17:17.240 --> 00:17:21.100
But I quickly get sort of too much noise,

293
00:17:21.100 --> 00:17:23.500
but I could also make a glyph now,

294
00:17:23.500 --> 00:17:27.150
a single plot for each baseball player to let me compare.

295
00:17:27.150 --> 00:17:31.050
So for example we could have drawn Gonzales, or Votto,

296
00:17:31.050 --> 00:17:36.540
or Infante, and see different sizes and shapes.

297
00:17:36.540 --> 00:17:39.250
And we can see Gonzales was very well rounded in all of these,

298
00:17:39.250 --> 00:17:45.125
while Infante may have been good at Batting Average but he had a low RBI for example.

299
00:17:45.125 --> 00:17:49.360
And these glyphs allow us to compare between different players.

300
00:17:49.360 --> 00:17:51.030
We could have done this for countries,

301
00:17:51.030 --> 00:17:53.615
we could do this for quiz scores in class,

302
00:17:53.615 --> 00:17:55.440
all sorts of different things.

303
00:17:55.440 --> 00:17:57.910
But we don't have to just stick with star plots,

304
00:17:57.910 --> 00:18:00.615
and with these single views.

305
00:18:00.615 --> 00:18:05.210
People have also tried this for other sorts of things.

306
00:18:05.210 --> 00:18:08.080
Thinking well, humans are really good at facial recognition.

307
00:18:08.080 --> 00:18:10.355
So what if I encode data into a face.

308
00:18:10.355 --> 00:18:13.320
Or what if I try to draw a plant

309
00:18:13.320 --> 00:18:18.040
with petals and stem rings and those sorts of things equal to the data.

310
00:18:18.040 --> 00:18:20.485
And so Chernoff Faces are an example,

311
00:18:20.485 --> 00:18:24.040
where we have a whole lot of different ways we can encode the data.

312
00:18:24.040 --> 00:18:28.550
So what if we have a car data set where we have variables on miles per gallon,

313
00:18:28.550 --> 00:18:33.500
the weight of the car, or the year every car was made, horsepower with cylinders?

314
00:18:33.500 --> 00:18:39.970
And so we can start thinking about drawing a face that represents this car data set.

315
00:18:39.970 --> 00:18:42.380
So for example, we could say okay,

316
00:18:42.380 --> 00:18:44.930
the further apart the eyes,

317
00:18:44.930 --> 00:18:47.375
the more miles per gallon we have.

318
00:18:47.375 --> 00:18:49.765
The more rounded the head is,

319
00:18:49.765 --> 00:18:51.535
the more the car weighs.

320
00:18:51.535 --> 00:18:53.970
The eye size could correspond to the year.

321
00:18:53.970 --> 00:18:58.030
The mouth will correspond the horsepower, and so forth.

322
00:18:58.030 --> 00:19:01.230
And so we can map our different variables to

323
00:19:01.230 --> 00:19:05.065
all of these different possible combinations for Chernoff Faces,

324
00:19:05.065 --> 00:19:07.485
and then for each car we get a different face.

325
00:19:07.485 --> 00:19:11.005
Then we can try to organize cars based on face similarity,

326
00:19:11.005 --> 00:19:13.205
to have different groups and clusters,

327
00:19:13.205 --> 00:19:16.565
and we can do the same thing with star plots as well.

328
00:19:16.565 --> 00:19:19.030
And so we can get this again,

329
00:19:19.030 --> 00:19:22.485
concept of small multiples,

330
00:19:22.485 --> 00:19:25.685
so each row in our data set has its own face.

331
00:19:25.685 --> 00:19:30.110
So we can start comparing these clustering and grouping together.

332
00:19:30.110 --> 00:19:35.835
And so hopefully you can see how scatter plots and parallel coordinate plots relate,

333
00:19:35.835 --> 00:19:40.145
how we can go and transfer these to other shapes such as star plots,

334
00:19:40.145 --> 00:19:44.490
and how we can even really think about different sorts of elements

335
00:19:44.490 --> 00:19:49.145
like a face for Chernoff Faces for encoding data to all sorts of things.

336
00:19:49.145 --> 00:19:53.770
And again we can think about this for a flower, right?

337
00:19:53.770 --> 00:19:58.150
So, we could have the number of petals could represent a particular variable,

338
00:19:58.150 --> 00:20:01.140
the distance, the angle between petals could represent things,

339
00:20:01.140 --> 00:20:03.735
the number of dots on the pedal face,

340
00:20:03.735 --> 00:20:07.010
and all sorts of different visual elements

341
00:20:07.010 --> 00:20:09.715
have been explored to try to help people look at,

342
00:20:09.715 --> 00:20:12.275
quickly overview, understand their data,

343
00:20:12.275 --> 00:20:14.785
and there's pros and cons to all of these.

344
00:20:14.785 --> 00:20:17.565
Some of the things like Chernoff Faces,

345
00:20:17.565 --> 00:20:21.250
there's papers trying to compare how well people interpret these.

346
00:20:21.250 --> 00:20:23.980
People have problems with interpreting some of these data.

347
00:20:23.980 --> 00:20:26.710
It's hard to get an exact value if I ask you exactly how

348
00:20:26.710 --> 00:20:30.495
far apart are the eyes spaced. It's hard to tell.

349
00:20:30.495 --> 00:20:33.880
But you can probably tell us which face,

350
00:20:33.880 --> 00:20:35.250
if I'd give you two different faces,

351
00:20:35.250 --> 00:20:37.910
which one has them further apart than the other,

352
00:20:37.910 --> 00:20:39.650
just not how much further.

353
00:20:39.650 --> 00:20:41.320
And so when I think about these trade offs,

354
00:20:41.320 --> 00:20:44.335
when I think about how we can compare correlations and data sets,

355
00:20:44.335 --> 00:20:46.145
how we can extract information,

356
00:20:46.145 --> 00:20:49.680
and what are other sorts of visualizations we might be able to use to

357
00:20:49.680 --> 00:20:55.000
explore and draw this multivariate data. Thank you.